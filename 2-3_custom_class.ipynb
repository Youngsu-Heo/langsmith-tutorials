{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Custom class\n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ as env\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env-leo', override=True)\n",
    "langsmith_key = env.get('LANGSMITH_API_KEY')\n",
    "openai_api_key = env.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPredictor:\n",
    "    state = 0    \n",
    "    def predict(input_: dict) -> dict:\n",
    "        MyPredictor.state += 1\n",
    "        return {\"output\": f\"Bar Bar Bar {MyPredictor.state}\"}\n",
    "\n",
    "def create_object(input):\n",
    "    return MyPredictor.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "dataset_name = \"Rap Battle Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'custom-class-test-1' at:\n",
      "https://smith.langchain.com/o/08d5427c-65f6-54e0-ba99-91184efa5aca/datasets/8257acae-3a07-4903-bb7c-9a7b93b8ff65/compare?selectedSessions=750523e9-ac7b-4144-a499-01bde7252a6b\n",
      "\n",
      "View all tests for Dataset Rap Battle Dataset at:\n",
      "https://smith.langchain.com/o/08d5427c-65f6-54e0-ba99-91184efa5aca/datasets/8257acae-3a07-4903-bb7c-9a7b93b8ff65\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'custom-class-test-1',\n",
       " 'results': {'c0b1a426-dbb1-4bc7-98ae-1d7137988fe9': {'input': {'question': 'a rap battle between Atticus Finch and Cicero'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\nThe input task is a rap battle between Atticus Finch and Cicero. \\n\\nThe submission is \"Bar Bar Bar 2.\"\\n\\nTo assess the submission against the criterion, we need to consider whether it is helpful, insightful, and appropriate.\\n\\nThe submission does not seem to be helpful or insightful as it does not provide any information or insight about a rap battle between Atticus Finch and Cicero. It is unclear what \"Bar Bar Bar 2\" means in this context.\\n\\nAs for appropriateness, while the submission is not offensive or inappropriate in a general sense, it does not seem to be appropriate to the specific task of a rap battle between Atticus Finch and Cicero.\\n\\nTherefore, the submission does not meet the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4d795cc-ba46-4baf-950a-76e144d6acb8'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is to assess whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, \"Bar Bar Bar 2\", it doesn\\'t contain any explicit, harmful, or offensive content. It doesn\\'t seem to be inappropriate either. It doesn\\'t seem to have any connection to the input, which is a rap battle between Atticus Finch and Cicero, but that\\'s not part of the criteria we\\'re assessing.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fe6c5182-e8fa-4f8c-ad88-9cf7ff25be38'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='cliche', score=0, value='N', comment='The criterion asks if the lyrics are cliche. The submission provided, \"Bar Bar Bar 2,\" does not contain any lyrics, let alone cliche ones. Therefore, it\\'s impossible to determine if the lyrics are cliche or not based on the submission. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('14792d30-1107-410f-91b3-8fbbfb6309ba'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.000244,\n",
       "   'run_id': '4293e0e4-01d1-4f8f-95d0-f0ead556045f',\n",
       "   'output': {'output': 'Bar Bar Bar 2'},\n",
       "   'reference': {'must_mention': ['lawyer', 'justice']}},\n",
       "  '148a4124-29c5-41cf-9cd5-386b83b8f1f5': {'input': {'question': 'a rap battle between Barbie and Oppenheimer'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\nThe input task is a rap battle between Barbie and Oppenheimer. \\n\\nThe submitted answer is \"Bar Bar Bar 1\". \\n\\nThis submission does not seem to be helpful or insightful as it does not provide any meaningful content related to the task. It does not depict a rap battle between Barbie and Oppenheimer. \\n\\nThe submission is not appropriate for the task as it does not fulfill the requirements of the task. \\n\\nTherefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('33e8049e-3cd0-44d5-84ba-d5ab31d86fb5'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is to assess whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, \"Bar Bar Bar 1\", it doesn\\'t contain any harmful, offensive, or inappropriate content. It doesn\\'t seem to be related to the input, which is a rap battle between Barbie and Oppenheimer, but that\\'s not part of the criteria we\\'re assessing.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('882d6b94-32fe-4276-86ee-ca10549ef197'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='cliche', score=0, value='N', comment='The criterion asks if the lyrics are cliche. The submission provided, \"Bar Bar Bar 1,\" does not contain any known cliches. It is also not clear how it relates to a rap battle between Barbie and Oppenheimer, but that is not part of the criteria. The criteria only asks about the presence of cliches. Therefore, the submission is not cliche.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2672ec36-00bb-4eb4-893d-4a9f4270b558'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.001079,\n",
       "   'run_id': 'bf8ebf3f-c66d-4fe4-ace8-ded1ef00075a',\n",
       "   'output': {'output': 'Bar Bar Bar 1'},\n",
       "   'reference': {'must_mention': ['plastic', 'nuclear']}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def must_mention(run, example) -> EvaluationResult:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    must_mention = example.outputs.get(\"must_mention\") or []\n",
    "    score = all(phrase in prediction for phrase in must_mention)\n",
    "    return EvaluationResult(key=\"must_mention\", score=score)\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[must_mention],\n",
    "    # You can also use a prebuilt evaluator\n",
    "    # by providing a name or RunEvalConfig.<configured evaluator>\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"cliche\": \"Are the lyrics cliche?\"\n",
    "                \" Respond Y if they are, N if they're entirely unique.\"\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    # We are passing the \"factory\" function in this case.\n",
    "    llm_or_chain_factory=create_object,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=\"custom-class-test-1\",\n",
    "    project_metadata={\"version\": \"1.0.0\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
