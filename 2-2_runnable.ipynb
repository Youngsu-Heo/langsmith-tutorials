{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Runnable (LCEL 문법)\n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ as env\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env-leo', override=True)\n",
    "langsmith_key = env.get('LANGSMITH_API_KEY')\n",
    "openai_api_key = env.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Spit some bars about {question}.\")])\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "dataset_name = \"Rap Battle Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-runnable-2024-03-24 02:37:03' at:\n",
      "https://smith.langchain.com/o/08d5427c-65f6-54e0-ba99-91184efa5aca/datasets/8257acae-3a07-4903-bb7c-9a7b93b8ff65/compare?selectedSessions=78e93c78-7044-451e-ba3a-a3a7712878a6\n",
      "\n",
      "View all tests for Dataset Rap Battle Dataset at:\n",
      "https://smith.langchain.com/o/08d5427c-65f6-54e0-ba99-91184efa5aca/datasets/8257acae-3a07-4903-bb7c-9a7b93b8ff65\n",
      "[------------------------------------------------->] 2/2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'test-runnable-2024-03-24 02:37:03',\n",
       " 'results': {'c0b1a426-dbb1-4bc7-98ae-1d7137988fe9': {'input': {'question': 'a rap battle between Atticus Finch and Cicero'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\n1. Helpfulness: The submission is a creative interpretation of the given task, which is a rap battle between Atticus Finch and Cicero. It provides a narrative of how such a battle might unfold, which can be considered helpful in understanding the task.\\n\\n2. Insightfulness: The submission demonstrates insight into the characters of Atticus Finch and Cicero, highlighting their respective strengths - Finch\\'s moral compass and Cicero\\'s rhetorical genius.\\n\\n3. Appropriateness: The submission is appropriate as it sticks to the task of creating a rap battle between the two characters. It does not include any offensive or inappropriate content.\\n\\nBased on this analysis, the submission meets all the criteria.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e458d259-346b-45f4-ad79-59e66d4b7b48'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a creative rap battle between two historical figures, Atticus Finch and Cicero. The language used is respectful and there are no offensive or inappropriate words or phrases. The content of the rap battle is also not harmful as it is a fictional scenario and does not promote any harmful actions or behaviors.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d88d2cc3-741e-4ba5-a1a0-acef62784f0a'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='cliche', score=1, value='Y', comment='The criterion asks if the lyrics are cliche. To determine this, we need to assess if the lyrics use overused or predictable phrases, themes, or ideas.\\n\\nThe lyrics do use some common themes and phrases often found in rap battles, such as \"spit fire in the sky\", \"each verse a weapon, each rhyme a blow\", and \"only one can emerge the victor\". These phrases are often used in rap battles to describe the intensity and competitive nature of the event.\\n\\nThe theme of the rap battle, two figures from history or literature battling it out with words, is also a common theme in this genre. The idea of one figure \"outshining\" the other and being declared the \"true champion\" is also a common trope.\\n\\nTherefore, based on the use of these common phrases and themes, the lyrics can be considered somewhat cliche.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e09293c0-0854-493c-9533-2ac2dd030770'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 6.086775,\n",
       "   'run_id': '8d52492d-800a-4e0a-94cf-1cb0c7ecc629',\n",
       "   'output': \"Atticus Finch, the lawyer with a cause\\nVersus Cicero, the orator with no flaws\\nIn the courtroom or on the stage\\nThese two legends are ready to engage\\n\\nFinch steps up, with his wisdom and grace\\nDefending the innocent, winning the case\\nCicero counters, with his words so sharp\\nRhetorical genius, tearing apart\\n\\nThe battle is fierce, the tension is high\\nAs Finch and Cicero spit fire in the sky\\nEach verse a weapon, each rhyme a blow\\nBut only one can emerge the victor, yo\\n\\nIn the end, it's a close call\\nBut Atticus Finch stands tall\\nWith his moral compass and righteous heart\\nHe outshines Cicero, tearing him apart\\n\\nSo raise a glass to the lawyer in the suit\\nAtticus Finch, the man who's resolute\\nIn the battle of words, he reigns supreme\\nA true champion, in this rap battle dream.\",\n",
       "   'reference': {'must_mention': ['lawyer', 'justice']}},\n",
       "  '148a4124-29c5-41cf-9cd5-386b83b8f1f5': {'input': {'question': 'a rap battle between Barbie and Oppenheimer'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness: Is the submission helpful, insightful, and appropriate?\"\\n\\n1. Helpfulness: The submission is a creative interpretation of the given task, which is a rap battle between Barbie and Oppenheimer. It provides a narrative of how such a battle might unfold, which could be helpful for someone trying to visualize or understand the concept.\\n\\n2. Insightfulness: The submission provides insight into the characters of Barbie and Oppenheimer, highlighting their contrasting attributes (beauty vs brains, style vs knowledge, etc.). It also offers a perspective on the purpose of the rap battle, suggesting that it\\'s not about who\\'s the best, but about the journey and the challenge.\\n\\n3. Appropriateness: The submission is appropriate for the task. It maintains a respectful tone towards both characters and doesn\\'t include any offensive or inappropriate content.\\n\\nBased on this analysis, the submission meets all the criteria. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('881485cb-9565-4002-852a-01492f042e41'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is harmfulness. The submission should not be harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a creative rap battle between Barbie and Oppenheimer. The content is playful and does not contain any harmful, offensive, or inappropriate language or themes. The rap battle is respectful and does not degrade either character. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e64b3ebc-edc8-474d-b430-3264811e3067'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='cliche', score=1, value='Y', comment='The criterion asks if the lyrics are cliche. To determine this, we need to assess if the lyrics use overused or predictable phrases, themes, or ideas. \\n\\nLooking at the lyrics, we can see that they use some common themes and phrases often found in rap battles. For example, the idea of a \"battle of wits\" or \"reaching for the sky\" are not particularly unique or innovative. The concept of a beauty versus brains clash is also a common trope. \\n\\nHowever, the specific combination of Barbie and Oppenheimer in a rap battle is quite unique. The lyrics also contain some unique elements, such as Barbie\\'s pink Corvette and Oppenheimer\\'s equations. \\n\\nDespite these unique elements, the overall structure and themes of the lyrics do contain some cliches. Therefore, the lyrics can be considered cliche to some extent.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('88c5eef2-91b1-41b2-8235-bc846f01fc0a'))}, source_run_id=None, target_run_id=None),\n",
       "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 2.947048,\n",
       "   'run_id': 'df3507b9-56f0-439b-b24a-903db0bbf41d',\n",
       "   'output': \"Barbie stepping up, looking all dolled up\\nOppenheimer in the lab, ready to blow things up\\nIt's a clash of worlds, beauty versus brains\\nBut don't underestimate Barbie, she's got some insane gains\\n\\nOppenheimer dropping knowledge, spitting fire like a bomb\\nBarbie coming back with style, she's the queen of the prom\\nIt's a battle of wits, who will come out on top\\nBarbie's got the looks, but Oppenheimer won't stop\\n\\nBarbie with her pink Corvette, Oppenheimer with his equations\\nIt's a showdown of epic proportions, no limitations\\nIn the end, it's not about who's the best\\nIt's about the journey, the challenge, the test\\n\\nSo let the battle rage on, let the words fly\\nBarbie and Oppenheimer, reaching for the sky\\nIn the end, they'll both come out stronger\\nBecause in the world of rap battles, there's no right or wronger.\",\n",
       "   'reference': {'must_mention': ['plastic', 'nuclear']}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
    "\n",
    "@run_evaluator\n",
    "def must_mention(run, example) -> EvaluationResult:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"must_mention\") or []\n",
    "    score = all(phrase in prediction for phrase in required)\n",
    "    return EvaluationResult(key=\"must_mention\", score=score)\n",
    "\n",
    "    \n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[must_mention],\n",
    "    # You can also use a prebuilt evaluator\n",
    "    # by providing a name or RunEvalConfig.<configured evaluator>\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"cliche\": \"Are the lyrics cliche?\"\n",
    "                \" Respond Y if they are, N if they're entirely unique.\"\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=chain,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    project_name=\"test-runnable\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    project_metadata={\"version\": \"1.0.0\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
